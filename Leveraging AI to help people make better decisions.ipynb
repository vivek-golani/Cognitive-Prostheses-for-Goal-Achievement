{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"Leveraging AI to help people make better decisions.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"iD_jlPE5v2M3"},"source":["### Leveraging AI to help people become more productive\n","\n","Problem Statement : Write a function that computes the optimal reward function (Equation 10) for the version of the flight planning problem shown in Figure 1A where the total number of flights is known (i.e., a finite horizon MDP with gamma=1). Your function should take the remaining number of flights and the current location as input arguments and return the incentives for the choices that are available in that location. Write tests for your code and demonstrate that it is working.\n","\n","\n","\n","Approach : I have considered the six locations in the flight planning problem as numbered from 0 to 5 starting from Jonesville as 0, followed by Smithsville as 1 and so on in a clockwise fashion with Williamsville as 5. Further a state is defined by two variables: location and number of flights remaining. The policy is to perform the action that has maximum rewards to get optimal state value functions.  \n"]},{"cell_type":"code","metadata":{"id":"B-PnjJMcv2NB"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YOxuUEZ7v2ND"},"source":["#declaring variables of MDP like states, actions etc.\n","STATES = 6\n","ACTIONS = 2\n","DISCOUNT = 1\n","\n","#assumed total number of flights as 20\n","TOTAL_FLIGHTS = 20\n","\n","#destinations from one state to another\n","DESTNS = [np.array([3,5]),\n","          np.array([0,4]),\n","          np.array([1,5]),\n","          np.array([1,2]),\n","          np.array([0,3]),\n","          np.array([2,4])]\n","\n","#rewards of different actions from a state\n","REWARDS = [np.array([-70,-30]),\n","           np.array([140,30]),\n","           np.array([-30,30]),\n","           np.array([-70,-30]),\n","           np.array([30,-30]),\n","           np.array([-70,-30])]\n","\n","#matrix to store optimal value functions of states\n","new_state_values = np.zeros((STATES, TOTAL_FLIGHTS + 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-iZGSLwv2NE"},"source":["#defining terminal state when the number of remaining flights is 0\n","def is_terminal(state):\n","    x, y = state\n","    return (y == 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8pxk_qwfv2NG"},"source":["#this function returns the reward and next state based on current state and action chosen\n","def step(state, action):\n","    if is_terminal(state):\n","        return state, 0\n","    \n","    x, y = state\n","    '''the next state is the destination according to the action chosen from current state \n","       and the flights remaining in the next location is one less than the current state '''\n","    next_state, reward = (DESTNS[x][action], y-1), REWARDS[x][action]\n","    \n","    return next_state, reward\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1C4fE7Ev2NH"},"source":["#this function is used to compute optimal state value functions\n","def compute_state_value():\n","    \n","    while True:\n","        #matrix to store state value functions from previous iteration\n","        state_values = new_state_values.copy()\n","        old_state_values = state_values.copy()\n","\n","        for i in range(STATES):\n","            for j in range(TOTAL_FLIGHTS + 1):\n","                value = 0\n","                #taking next states and rewards based on both actions possible from current state\n","                (next_i1, next_j1), reward1 = step([i, j], 0)\n","                (next_i2, next_j2), reward2 = step([i, j], 1)\n","                \n","                #calculating discounted rewards of both actions and optimal policy is choosing action with maximum returns.\n","                rew1 = reward1 + DISCOUNT * state_values[next_i1, next_j1]\n","                rew2 = reward2 + DISCOUNT * state_values[next_i2, next_j2]\n","                if rew1 > rew2 :\n","                    value = rew1\n","                else :\n","                    value = rew2\n","                \n","                #updating the matrix that stores optimal state value functions\n","                new_state_values[i, j] = value\n","        \n","        #terminating condition is that the state values are now being updated by values less than 10^-4 which is insignificant\n","        max_delta_value = abs(old_state_values - new_state_values).max()\n","        if max_delta_value < 1e-4:\n","            break\n","\n","    print(new_state_values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7E6uaqT9v2NI"},"source":["def incentives(rem_no_of_flights, location) :\n","    \n","    #calculating optimal rewards as stated in equation 9 in the research paper.\n","    if rem_no_of_flights > 0 :\n","        rew1 = REWARDS[location][0] + DISCOUNT * new_state_values[DESTNS[location][0], rem_no_of_flights-1] - new_state_values[location, rem_no_of_flights]\n","        rew2 = REWARDS[location][1] + DISCOUNT * new_state_values[DESTNS[location][1], rem_no_of_flights-1] - new_state_values[location, rem_no_of_flights]\n","    \n","    else : \n","        rew1 = REWARDS[location][0] \n","        rew2 = REWARDS[location][1] \n","        \n","    print( rew1, rew2 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"bDFGm3kWv2NL","outputId":"60f99c9b-632d-43d7-c75a-0cc6501e0ca3"},"source":["if __name__ == '__main__':\n","    compute_state_value()\n","    incentives(10,1)\n","    incentives(1,3)\n","    incentives(15,5)\n","    incentives(5,4)\n","    incentives(0,0)\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[[  0. -30. -60.   0.  10. -20.   0.  10.  20.   0.  10.  20.  30.  10.\n","   20.  30.  40.  20.  30.  40.  50.]\n"," [  0. 140. 110.  80. 140. 150. 120. 140. 150. 160. 140. 150. 160. 170.\n","  150. 160. 170. 180. 160. 170. 180.]\n"," [  0.  30. 110.  80.  70. 110. 120.  90. 110. 120. 130. 110. 120. 130.\n","  140. 120. 130. 140. 150. 130. 140.]\n"," [  0. -30.  70.  80.  50.  70.  80.  90.  70.  80.  90. 100.  80.  90.\n","  100. 110.  90. 100. 110. 120. 100.]\n"," [  0.  30.   0.  40.  50.  40.  40.  50.  60.  50.  50.  60.  70.  60.\n","   60.  70.  80.  70.  70.  80.  90.]\n"," [  0. -30.   0.  40.  10.  20.  40.  50.  20.  40.  50.  60.  40.  50.\n","   60.  70.  50.  60.  70.  80.  60.]]\n","0.0 -60.0\n","-40.0 0.0\n","0.0 -40.0\n","0.0 -20.0\n","-70 -30\n"]}]}]}